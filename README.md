# Q-Learning ‚Äì Projeto de Aprendizado por Refor√ßo

Este projeto implementa o algoritmo **Q-Learning** para ensinar um agente a controlar o personagem **Amongois** em um jogo. O objetivo √© que o agente aprenda, por tentativa e erro, o melhor caminho para alcan√ßar o **bloco preto**, evitando quedas e minimizando penalidades.

---

## Como Executar o Projeto

### 1. Inicie o Jogo

- Navegue at√© a pasta onde o execut√°vel do jogo est√° localizado.
- Abra o arquivo `.exe` para iniciar o servidor local do jogo.

> Isso abrir√° uma janela com o ambiente do jogo. O agente ser√° controlado pelo script Python via socket.

### 2. Execute o Script de Treinamento

- No terminal, com o ambiente Python ativado, execute:

```bash
python treinamento.py
```

O script ir√°:

- Conectar ao servidor do jogo.
- Carregar (ou continuar atualizando) a Q-table salva no arquivo `resultado.txt`.
- Treinar o agente atrav√©s de epis√≥dios infinitos at√© ser interrompido manualmente.
- Salvar a Q-table a cada epis√≥dio no arquivo `resultado.txt`.

> O pr√≥prio arquivo `treinamento.py` cont√©m, ao final, um script de simula√ß√£o comentado. Basta descomentar para testar a pol√≠tica aprendida (com `epsilon = 0`) sem mais treinamento.

---

## Como Funciona o Q-Learning

O Q-Learning √© uma t√©cnica de **aprendizado por refor√ßo**. O agente interage com o ambiente e aprende, por tentativa e erro, a melhor a√ß√£o a tomar em cada estado com base nas recompensas que recebe.

A estrat√©gia segue a equa√ß√£o de Bellman:

```
Q(s, a) ‚Üê (1 - Œ±) * Q(s, a) + Œ± * (reward + Œ≥ * max(Q(s', a')))
```

- `Œ±` (alpha): taxa de aprendizado ‚Äî quanto o agente aprende a cada itera√ß√£o.
- `Œ≥` (gamma): fator de desconto ‚Äî quanto ele valoriza recompensas futuras.
- `reward`: recompensa recebida pela a√ß√£o.
- `s` / `s'`: estado atual e estado seguinte.
- `a`: a√ß√£o tomada.

O agente usa uma **Q-table** para armazenar o valor esperado de cada a√ß√£o em cada estado. Com o tempo, ele aprende quais a√ß√µes o levam mais rapidamente ao objetivo (recompensa +300) e evita caminhos que levam √† morte (recompensa -100).

---

## Estrat√©gia de Aprendizado

- **Estados** s√£o representados por n√∫meros inteiros (convertidos do bin√°rio do jogo que indica a posi√ß√£o e dire√ß√£o do personagem).
- **A√ß√µes dispon√≠veis:** `"left"`, `"right"` e `"jump"`.
- **Recompensas:**
  - Recompensas negativas (ex: `-14`) se o personagem avan√ßa sem sucesso.
  - Recompensa positiva de `300` se alcan√ßar o objetivo.
  - Recompensa de `-100` se morrer (cair no vazio).

### Pol√≠tica Epsilon-Greedy:

- O agente escolhe uma a√ß√£o aleat√≥ria com probabilidade `epsilon` (explora√ß√£o).
- Caso contr√°rio, escolhe a melhor a√ß√£o segundo a Q-table (explora√ß√£o).

### Decaimento e Rein√≠cio de `epsilon`:

- `epsilon` come√ßa em `0.2` e decai gradualmente (`decay_rate = 0.995`) at√© `0.05`.
- Quando chega ao m√≠nimo, ele √© **reiniciado para `0.5`**, permitindo novas explora√ß√µes e evitando estagna√ß√£o.

---

## üóÇÔ∏è Arquivos do Projeto

| Arquivo         | Descri√ß√£o |
|------------------|-----------|
| `treinamento.py` | Script principal que realiza o treinamento do agente. Inclui, ao final, um bloco comentado com o modo de simula√ß√£o para avalia√ß√£o da pol√≠tica aprendida. |
| `connection.py`  | Interface com o jogo: conecta ao servidor e envia/recebe a√ß√µes e estados. |
| `resultado.txt`  | Arquivo com a Q-table aprendida. Salvo e atualizado automaticamente ap√≥s cada epis√≥dio. |

---

## ‚öôÔ∏è Hiperpar√¢metros do Algoritmo

| Par√¢metro                       | Valor  |
|---------------------------------|--------|
| `alpha` (taxa de aprendizado)   | 0.6    |
| `gamma` (fator de desconto)     | 0.9    |
| `epsilon` inicial               | 0.2    |
| `min_epsilon`                   | 0.05   |
| `decay_rate`                    | 0.995  |
| `epsilon_reset`                 | 0.5    |

---

## Resultados

- Em testes de simula√ß√£o (com `epsilon = 0`), o agente atingiu o objetivo em:
  - ‚úÖ 11 de 20 epis√≥dios (teste 1)
  - ‚úÖ 15 de 20 epis√≥dios (teste 2)

Esses resultados indicam que a pol√≠tica aprendida √© eficaz, mesmo sem atingir 100% de sucesso.

---

## Entreg√°veis

- `treinamento.py`
- `connection.py`
- `resultado.txt` (Q-table aprendida)
- `video.mp4` (apresenta√ß√£o do projeto e execu√ß√£o da pol√≠tica)

---

## Grupo

- Ellian dos Santos Rodrigues
- Guilherme Ribeiro Costa Carvalho
- Guilherme Cezar Menezes Siqueira