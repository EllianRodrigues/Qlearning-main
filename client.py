from connection import connect, get_state_reward
import random
import numpy as np

# Conecta ao jogo
server_socket = connect(2037)

# Carrega a Q-table existente
utility_matrix = np.loadtxt('resultado.txt')
print("‚úÖ Matriz de utilidade carregada.")

np.set_printoptions(precision=6)

actions = ["left", "right", "jump"]

# Hiperpar√¢metros
alpha = 0.6               # taxa de aprendizado
gamma = 0.9               # fator de desconto
epsilon = 0.2             # taxa de explora√ß√£o inicial
min_epsilon = 0.05        # m√≠nimo para epsilon
decay_rate = 0.995        # taxa de decaimento por passo

# Estat√≠sticas
successes = 0
episodes = 0

while True:
    print(f"\nüîÑ Iniciando epis√≥dio {episodes + 1}")
    state = 0
    reward = -14
    total_reward = 0

    while True:
        print(f"\nEstado atual: {state}")
        print(f"Taxa de explora√ß√£o (Œµ): {epsilon:.3f}")

        # Escolha da a√ß√£o
        if random.random() < epsilon:
            action_index = random.randint(0, 2)
            chosen_action = actions[action_index]
            print(f"A√ß√£o escolhida ALEAT√ìRIA: {chosen_action}")
        else:
            action_index = np.argmax(utility_matrix[state])
            chosen_action = actions[action_index]
            print(f"A√ß√£o escolhida pelo AGENTE: {chosen_action}")

        # Executa a√ß√£o
        state_info, reward = get_state_reward(server_socket, chosen_action)
        next_state = int(state_info[2:], 2)
        total_reward += reward

        # Atualiza Q-table (Bellman)
        old_value = utility_matrix[state][action_index]
        next_max = np.max(utility_matrix[next_state])
        utility_matrix[state][action_index] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)

        print(f"Recompensa: {reward}")
        print(f"Pr√≥ximo estado: {next_state}")
        print(f"Valor Q antes: {old_value:.6f}")
        print(f"Valor Q depois: {utility_matrix[state][action_index]:.6f}")

        # Atualiza estado
        state = next_state

        # Atualiza epsilon
        epsilon = max(min_epsilon, epsilon * decay_rate)
        if epsilon == min_epsilon:
            epsilon = 0.5  # reinicia se chegar no m√≠nimo

        # Termina epis√≥dio se morrer ou chegar no objetivo
        if reward == -100:
            print("üíÄ Personagem morreu.")
            break
        elif reward == 300:
            print("üéâ Objetivo atingido!")
            successes += 1
            break

    # Salva Q-table ap√≥s cada epis√≥dio
    np.savetxt('resultado.txt', utility_matrix, fmt="%.6f")

    episodes += 1
    print(f"üéØ Epis√≥dios finalizados: {episodes} | Sucessos: {successes} | Taxa de acerto: {(successes / episodes) * 100:.2f}%")

'''
# Script de teste com a Q-table aprendida

from connection import connect, get_state_reward
import numpy as np

# Conecta ao jogo
server_socket = connect(2037)

# Carrega a Q-table aprendida
utility_matrix = np.loadtxt('resultado.txt')
print("‚úÖ Q-table carregada com sucesso para simula√ß√£o.")

np.set_printoptions(precision=6)
actions = ["left", "right", "jump"]

# Par√¢metros da simula√ß√£o
exploration_rate = 0.0  # zero explora√ß√£o
max_episodes = 20
successes = 0

for episode in range(max_episodes):
    print(f"\nüîÅ Epis√≥dio {episode + 1}")
    state = 0
    reward = -14
    steps = 0

    while True:
        print(f"\nüìç Estado atual: {state}")
        print(f"üéØ A√ß√£o escolhida: ", end="")

        # Escolhe a melhor a√ß√£o segundo a Q-table
        action_index = np.argmax(utility_matrix[state])
        action = actions[action_index]
        print(f"{action}")

        state_info, reward = get_state_reward(server_socket, action)
        print(f"üèÖ Recompensa: {reward}, Novo estado (bin√°rio): {state_info}")

        next_state = int(state_info[2:], 2)
        state = next_state
        steps += 1

        if reward == 300:
            print(f"\n‚úÖ Objetivo atingido em {steps} passos!")
            successes += 1
            break

        if reward == -100:
            print(f"\nüíÄ Personagem morreu ap√≥s {steps} passos.")
            break

# Resultado final
print("\nüìä RESULTADO DA SIMULA√á√ÉO FINAL")
print(f"Total de epis√≥dios: {max_episodes}")
print(f"Total de sucessos (chegou ao objetivo): {successes}")
print(f"Porcentagem de sucesso: {(successes / max_episodes) * 100:.2f}%")
'''